import SparkMD5 from 'spark-md5';

/**
 * åˆ†ç‰‡æ¥å£
 */
interface Chunk {
  blob: Blob;
  start: number;
  end: number;
  hash: string;
  index: number;
}

/**
 * è®¡ç®—chunkçš„hashå€¼
 */
function calcChunkHash(chunk: Chunk): Promise<string> {
  return new Promise((resolve) => {
    const spark = new SparkMD5.ArrayBuffer();
    const fileReader = new FileReader();
    
    fileReader.onload = (e) => {
      spark.append(e.target?.result as ArrayBuffer);
      resolve(spark.end());
    };
    
    fileReader.onerror = () => {
      resolve('');
    };
    
    fileReader.readAsArrayBuffer(chunk.blob);
  });
}

/**
 * Web Worker ç”¨äºè®¡ç®—åˆ†ç‰‡hash
 */
onmessage = async function (e) {
  console.log('ğŸ”§ SplitWorker æ”¶åˆ°æ¶ˆæ¯:', e.data.length, 'ä¸ªåˆ†ç‰‡');
  const chunks = e.data as Chunk[];
  const results: Chunk[] = [];
  
  for (const chunk of chunks) {
    try {
      console.log('ğŸ”§ è®¡ç®—åˆ†ç‰‡hash:', chunk.index, 'å¤§å°:', chunk.blob.size);
      const hash = await calcChunkHash(chunk);
      chunk.hash = hash;
      console.log('ğŸ”§ åˆ†ç‰‡hashè®¡ç®—å®Œæˆ:', chunk.index, 'hash:', hash);
      results.push(chunk);
    } catch (error) {
      console.error('âŒ è®¡ç®—åˆ†ç‰‡hashå¤±è´¥:', error);
      // å³ä½¿å¤±è´¥ä¹Ÿè¦æ¨é€ï¼Œé¿å…é˜»å¡
      results.push(chunk);
    }
  }
  
  console.log('ğŸ”§ SplitWorker è¿”å›ç»“æœ:', results.length, 'ä¸ªåˆ†ç‰‡');
  postMessage(results);
};
